{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71264b9-3685-4a84-8a3a-cf152760c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 8 - CNN- LSTM Model\n",
    "# Define the RNNCNN1 class that inherits from ModelBuilder\n",
    "class RNNCNN1(ModelBuilder):\n",
    "\n",
    "    # Define the model structure with parameters for LSTM cells, dense layer neurons, and dropout rate\n",
    "    def define_model(self, lstm_cells=64, dense_neurons=64, dropout=0.25):\n",
    "\n",
    "        # Initialize a sequential model\n",
    "        model = Sequential()\n",
    "\n",
    "        # Add the first TimeDistributed Conv2D layer with 16 filters, kernel size of 3x3, 'same' padding, and ReLU activation\n",
    "        model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "                                  input_shape=(self.frames_to_sample, self.image_height, self.image_width, self.channels)))\n",
    "        model.add(TimeDistributed(BatchNormalization()))  # Add TimeDistributed BatchNormalization\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))  # Add TimeDistributed MaxPooling2D with pool size of 2x2\n",
    "        \n",
    "        # Add the second TimeDistributed Conv2D layer with 32 filters and kernel size of 3x3\n",
    "        model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))  # Add TimeDistributed BatchNormalization\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))  # Add TimeDistributed MaxPooling2D with pool size of 2x2\n",
    "        \n",
    "        # Add the third TimeDistributed Conv2D layer with 64 filters and kernel size of 3x3\n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))  # Add TimeDistributed BatchNormalization\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))  # Add TimeDistributed MaxPooling2D with pool size of 2x2\n",
    "        \n",
    "        # Add the fourth TimeDistributed Conv2D layer with 128 filters and kernel size of 3x3\n",
    "        model.add(TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))  # Add TimeDistributed BatchNormalization\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))  # Add TimeDistributed MaxPooling2D with pool size of 2x2\n",
    "        \n",
    "        # Add the fifth TimeDistributed Conv2D layer with 256 filters and kernel size of 3x3\n",
    "        model.add(TimeDistributed(Conv2D(256, (3, 3), padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))  # Add TimeDistributed BatchNormalization\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))  # Add TimeDistributed MaxPooling2D with pool size of 2x2\n",
    "        \n",
    "        # Flatten the output from the convolutional layers to feed into the LSTM layer\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        # Add LSTM layer with the specified number of LSTM cells\n",
    "        model.add(LSTM(lstm_cells))\n",
    "        model.add(Dropout(dropout))  # Add dropout for regularization\n",
    "        \n",
    "        # Add a dense layer with the specified number of neurons and ReLU activation\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout))  # Add dropout for regularization\n",
    "        \n",
    "        # Add the output layer with the number of classes and softmax activation for classification\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        # Define the optimizer and compile the model with categorical crossentropy loss and accuracy metric\n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        # Return the compiled model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c521244-3874-4e02-9230-60c8cc5c811d",
   "metadata": {},
   "source": [
    "This code defines a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) network within the RNNCNN1 class, inheriting from ModelBuilder. Here's a breakdown of the code:\n",
    "\n",
    "**Key Features:**\n",
    "- **1.TimeDistributed Layers:**\n",
    "- Wraps CNN layers to process sequential data (video frames in this context).\n",
    "- ** 2.CNN Component:**\n",
    "- Multiple Conv2D layers with increasing filter counts (16, 32, 64, 128, 256) for hierarchical feature extraction.\n",
    "- Batch Normalization (BatchNormalization) after each Conv2D layer for faster convergence.\n",
    "- Max Pooling (MaxPooling2D) for spatial dimensionality reduction.\n",
    "- **3.Flattening and Transition to RNN:**\n",
    "- After the CNN feature extraction, outputs are flattened using TimeDistributed(Flatten()).\n",
    "- **4.LSTM Layer:**\n",
    "- Processes the sequential features extracted from the CNN layers.\n",
    "\n",
    "- Number of LSTM cells is configurable through lstm_cells.\n",
    "- **5.Dense Layers and Dropout:**\n",
    "- Dense layer for classification after LSTM with a configurable number of neurons (dense_neurons).\n",
    "- Dropout is applied after LSTM and dense layers for regularization.\n",
    "- **6.Output Layer:**\n",
    "- Final dense layer with softmax activation for multi-class classification (num_classes classes).\n",
    "- **7.Compilation:**\n",
    "- Adam optimizer.\n",
    "- categorical_crossentropy loss and categorical_accuracy metrics.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2f1501-325c-49d3-a180-e04fc6f43070",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cnn1=RNNCNN1()\n",
    "rnn_cnn1.initialize_path(project_folder)\n",
    "rnn_cnn1.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn1.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
    "rnn_cnn1_model=rnn_cnn1.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn1_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78581a10-368d-4832-a40e-17a7ba8ecc42",
   "metadata": {},
   "source": [
    "- **the code initializes, configures, and defines an RNN-CNN model for processing sequential image data, then prints its architecture summary..**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717352f-597f-4b13-8cc3-e2546c5f76a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Params:\", rnn_cnn1_model.count_params())\n",
    "history_model8=rnn_cnn1.train_model(rnn_cnn1_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3689835-faae-4590-becb-0f0c50c36365",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_model8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c4784-9473-4182-992e-4dceef18a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ModelBuilderMoreAugmentation class with ABCMeta as metaclass\n",
    "class ModelBuilderMoreAugmentation(metaclass=abc.ABCMeta):\n",
    "    \n",
    "    # Initialize paths for training and validation data\n",
    "    def initialize_path(self, project_folder):\n",
    "        # Shuffle and read training and validation file lists\n",
    "        self.train_doc = np.random.permutation(open(project_folder + '/' + 'train.csv').readlines())\n",
    "        self.val_doc = np.random.permutation(open(project_folder + '/' + 'val.csv').readlines())\n",
    "        self.train_path = project_folder + '/' + 'train'\n",
    "        self.val_path = project_folder + '/' + 'val'\n",
    "        self.num_train_sequences = len(self.train_doc)\n",
    "        self.num_val_sequences = len(self.val_doc)\n",
    "        \n",
    "    # Initialize image properties like height, width, channels, classes, and total frames\n",
    "    def initialize_image_properties(self, image_height=100, image_width=100):\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.channels = 3\n",
    "        self.num_classes = 5\n",
    "        self.total_frames = 30\n",
    "          \n",
    "    # Initialize hyperparameters like frames to sample, batch size, and number of epochs\n",
    "    def initialize_hyperparams(self, frames_to_sample=30, batch_size=20, num_epochs=20):\n",
    "        self.frames_to_sample = frames_to_sample\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    # Data generator function to yield batches of data and labels\n",
    "    def generator(self, source_path, folder_list, augment=False):\n",
    "        img_idx = np.round(np.linspace(0, self.total_frames-1, self.frames_to_sample)).astype(int)\n",
    "        batch_size = self.batch_size\n",
    "        while True:\n",
    "            t = np.random.permutation(folder_list)\n",
    "            num_batches = len(t) // batch_size\n",
    "        \n",
    "            for batch in range(num_batches): \n",
    "                batch_data, batch_labels = self.one_batch_data(source_path, t, batch, batch_size, img_idx, augment)\n",
    "                yield batch_data, batch_labels \n",
    "\n",
    "            remaining_seq = len(t) % batch_size\n",
    "        \n",
    "            if remaining_seq != 0:\n",
    "                batch_data, batch_labels = self.one_batch_data(source_path, t, num_batches, batch_size, img_idx, augment, remaining_seq)\n",
    "                yield batch_data, batch_labels \n",
    "    \n",
    "    # Prepare one batch of data with optional augmentation\n",
    "    def one_batch_data(self, source_path, t, batch, batch_size, img_idx, augment, remaining_seq=0):\n",
    "    \n",
    "        seq_len = remaining_seq if remaining_seq else batch_size\n",
    "    \n",
    "        batch_data = np.zeros((seq_len, len(img_idx), self.image_height, self.image_width, self.channels)) \n",
    "        batch_labels = np.zeros((seq_len, self.num_classes)) \n",
    "    \n",
    "        if augment:\n",
    "            batch_data_aug = np.zeros((seq_len, len(img_idx), self.image_height, self.image_width, self.channels))\n",
    "\n",
    "        for folder in range(seq_len): \n",
    "            imgs = os.listdir(source_path + '/' + t[folder + (batch * batch_size)].split(';')[0]) \n",
    "            for idx, item in enumerate(img_idx): \n",
    "                image = imageio.imread(source_path + '/' + t[folder + (batch * batch_size)].strip().split(';')[0] + '/' + imgs[item]).astype(np.float32)\n",
    "                image_resized = resize(image, (self.image_height, self.image_width, 3))\n",
    "\n",
    "                batch_data[folder, idx, :, :, 0] = image_resized[:, :, 0] / 255\n",
    "                batch_data[folder, idx, :, :, 1] = image_resized[:, :, 1] / 255\n",
    "                batch_data[folder, idx, :, :, 2] = image_resized[:, :, 2] / 255\n",
    "            \n",
    "                if augment:\n",
    "                    shifted = cv2.warpAffine(image, \n",
    "                                             np.float32([[1, 0, np.random.randint(-30, 30)], [0, 1, np.random.randint(-30, 30)]]), \n",
    "                                             (image.shape[1], image.shape[0]))\n",
    "                    \n",
    "                    gray = cv2.cvtColor(shifted, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                    x0, y0 = np.argwhere(gray > 0).min(axis=0)\n",
    "                    x1, y1 = np.argwhere(gray > 0).max(axis=0) \n",
    "                    \n",
    "                    cropped = shifted[x0:x1, y0:y1, :]\n",
    "                    \n",
    "                    image_resized = resize(cropped, (self.image_height, self.image_width, 3))\n",
    "                    \n",
    "                    M = cv2.getRotationMatrix2D((self.image_width // 2, self.image_height // 2),\n",
    "                                                np.random.randint(-10, 10), 1.0)\n",
    "                    rotated = cv2.warpAffine(image_resized, M, (self.image_width, self.image_height))\n",
    "                    \n",
    "                    batch_data_aug[folder, idx, :, :, 0] = rotated[:, :, 0] / 255\n",
    "                    batch_data_aug[folder, idx, :, :, 1] = rotated[:, :, 1] / 255\n",
    "                    batch_data_aug[folder, idx, :, :, 2] = rotated[:, :, 2] / 255\n",
    "            \n",
    "            batch_labels[folder, int(t[folder + (batch * batch_size)].strip().split(';')[2])] = 1\n",
    "    \n",
    "        if augment:\n",
    "            batch_data = np.concatenate([batch_data, batch_data_aug])\n",
    "            batch_labels = np.concatenate([batch_labels, batch_labels])\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    # Train the model using the generator, with optional data augmentation\n",
    "    def train_model(self, model, augment_data=False):\n",
    "        train_generator = self.generator(self.train_path, self.train_doc, augment=augment_data)\n",
    "        val_generator = self.generator(self.val_path, self.val_doc)\n",
    "\n",
    "        model_name = 'model_init' + '_' + str(datetime.datetime.now()).replace(' ', '').replace(':', '_') + '/'\n",
    "    \n",
    "        if not os.path.exists(model_name):\n",
    "            os.mkdir(model_name)\n",
    "        \n",
    "        filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "        LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4)\n",
    "        callbacks_list = [checkpoint, LR]\n",
    "\n",
    "        if (self.num_train_sequences % self.batch_size) == 0:\n",
    "            steps_per_epoch = int(self.num_train_sequences / self.batch_size)\n",
    "        else:\n",
    "            steps_per_epoch = (self.num_train_sequences // self.batch_size) + 1\n",
    "\n",
    "        if (self.num_val_sequences % self.batch_size) == 0:\n",
    "            validation_steps = int(self.num_val_sequences / self.batch_size)\n",
    "        else:\n",
    "            validation_steps = (self.num_val_sequences // self.batch_size) + 1\n",
    "    \n",
    "        history = model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=self.num_epochs, verbose=1, \n",
    "                                      callbacks=callbacks_list, validation_data=val_generator, \n",
    "                                      validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n",
    "        return history\n",
    "\n",
    "    # Abstract method to define the model, to be implemented by subclasses\n",
    "    @abc.abstractmethod\n",
    "    def define_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67778ec8-9343-4c2c-8cf6-b654fff95535",
   "metadata": {},
   "source": [
    "- **Objective:** The ModelBuilderMoreAugmentation class provides a framework to build and train deep learning models for image-based tasks, with built-in support for data augmentation and generators to handle large datasets efficiently.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    " **1.Initialization Functions:**\n",
    "\n",
    "- initialize_path(project_folder): Reads training and validation datasets (train.csv and val.csv), sets file paths, and calculates the number of sequences.\n",
    "- initialize_image_properties(image_height, image_width): Sets image resolution, channels, classes, and total frames per sequence.\n",
    "- initialize_hyperparams(frames_to_sample, batch_size, num_epochs): Configures hyperparameters such as the number of frames, batch size, and training epochs.\n",
    "  \n",
    " **2.Data Generator (generator):**\n",
    "\n",
    "- Dynamically loads and processes batches of data to avoid memory overload.\n",
    "- Performs optional augmentation (e.g., translation, cropping, rotation).\n",
    "- Yields batches of data and labels to the training loop.\n",
    "  \n",
    " **3. Augmentation Logic (one_batch_data):**\n",
    "\n",
    "- Applies random transformations like shifting, cropping, and rotation to augment training data.\n",
    "- Ensures each augmented batch has corresponding labels.\n",
    "  \n",
    " **4.Model Training (train_model):**\n",
    "\n",
    "- Trains the model using the Keras fit_generator method with training and validation generators.\n",
    "- Includes callbacks such as ModelCheckpoint for saving models and ReduceLROnPlateau for dynamic learning rate adjustments.\n",
    "  \n",
    " **5.Abstract Method (define_model):**\n",
    "\n",
    "- Provides a blueprint for subclasses to implement specific model architectures.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cf4dc3-df9f-4f74-88d8-b94aab59ceee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the test generator\n",
    "test_generator = Test()\n",
    "\n",
    "# Initialize paths, image properties, and hyperparameters\n",
    "test_generator.initialize_path(project_folder)\n",
    "test_generator.initialize_image_properties(image_height=160, image_width=160)\n",
    "test_generator.initialize_hyperparams(frames_to_sample=30, batch_size=3, num_epochs=1)\n",
    "\n",
    "# Generate a batch of data with augmentation\n",
    "g = test_generator.generator(test_generator.val_path, test_generator.val_doc, augment=True)\n",
    "batch_data, batch_labels = next(g)\n",
    "\n",
    "# Visualize the augmented data\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "axes[0].imshow(batch_data[0, 29, :, :, :])\n",
    "axes[1].imshow(batch_data[3, 29, :, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3876a75-942c-4b93-968e-6effb45c551e",
   "metadata": {},
   "source": [
    "-- **Objective:**\n",
    "- Evaluate the data generators ability to preprocess and augment image data for a sequence-based deep learning task.\n",
    "  **Steps:**\n",
    "\n",
    "**1.Setup:**\n",
    "\n",
    "- Created an instance of the Test generator class.\n",
    "- Initialized paths, image properties, and hyperparameters for the test dataset.\n",
    "  \n",
    "**2.Data Generation:**\n",
    "\n",
    "- Used the generator to fetch a batch of augmented validation data.\n",
    "- Retrieved batch_data (augmented images) and batch_labels (corresponding labels).\n",
    "\n",
    "**3.Visualization:**\n",
    "\n",
    "- Displayed two augmented frames from the batch to confirm transformations (e.g., cropping, rotation).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ceeedc-cee6-453d-b2a9-2cc542eabd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    " Model 9 is comparable to Model 2 with its 160x160 image resolution and augmentation (3,3,3) filter.\n",
    "\n",
    "# Define the ModelConv3D9 class inheriting from ModelBuilderMoreAugmentation\n",
    "class ModelConv3D9(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    # Define the model with parameters for filter size, dense neurons, and dropout rate\n",
    "    def define_model(self, filter_size=(3, 3, 3), dense_neurons=64, dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # First Conv3D layer with 16 filters\n",
    "        model.add(Conv3D(16, filter_size, padding='same',\n",
    "                         input_shape=(self.frames_to_sample, self.image_height, self.image_width, self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Second Conv3D layer with 32 filters\n",
    "        model.add(Conv3D(32, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Third Conv3D layer with 64 filters\n",
    "        model.add(Conv3D(64, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Fourth Conv3D layer with 128 filters\n",
    "        model.add(Conv3D(128, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Flatten the 3D output to 1D\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # First fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Second fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer with softmax activation for multi-class classification\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        # Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbda988-0ccb-495d-97d5-4babe1409b7d",
   "metadata": {},
   "source": [
    "**Expected Changes\n",
    " **1.Higher Resolution (160x160):**\n",
    "\n",
    "- **Impact:** Captures more detailed spatial information, potentially improving accuracy, especially for datasets where fine-grained features are important.\n",
    "- **Trade-off:** Increased computational cost and memory usage.\n",
    "  \n",
    " **Smaller Filter Size (3x3x3):**\n",
    "\n",
    "- **Impact:** Focuses on local features with smaller receptive fields, improving feature extraction for detailed patterns.\n",
    "- **Trade-off:** May require more layers to capture global patterns effectively.\n",
    "  \n",
    " **Additional Conv3D Layers:**\n",
    "\n",
    "- **Impact:** Allows the model to learn more complex feature hierarchies by stacking layers.\n",
    "- **Trade-off:** Increased training time and potential risk of overfitting if the dataset is small.\n",
    "  \n",
    " **Dense Layers and Dropout:**\n",
    "\n",
    "- **Impact:** Adding two dense layers with a smaller dropout rate (0.25) can enhance feature learning and reduce underfitting.\n",
    "- **Trade-off:** May slightly increase overfitting risk if dropout is too low.\n",
    "  \n",
    " **Augmentation (Model 9):**\n",
    "\n",
    "- **Impact:** Augmentation in Model 9 introduces robustness to variations (e.g., translation, rotation), likely improving generalization.\n",
    "- **Trade-off:** Requires additional computation during data preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fff6f8-3406-4546-bebe-c3cc88f5ad2d",
   "metadata": {},
   "source": [
    "**Applications and Use Cases**\n",
    "- **Model 8:** Suitable for scenarios with limited computational resources or datasets with lower resolution or less variation.\n",
    "- **Model 9:** Designed for larger, more complex datasets where higher resolution and robust augmentation can enhance performance.\n",
    "\n",
    "**Conclusion**\n",
    "ModelConv3D9 is an improved and more computationally intensive version of ModelConv3D8, aiming for better performance through higher resolution, smaller filters, additional layers, and aggressive augmentation. It is ideal for scenarios requiring higher accuracy and robust feature learning. However, it demands more resources and careful regularization to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af387d5a-24cd-4859-b756-6a464bb333c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_3d9=ModelConv3D9()\n",
    "conv_3d9.initialize_path(project_folder)\n",
    "conv_3d9.initialize_image_properties(image_height=160,image_width=160)\n",
    "conv_3d9.initialize_hyperparams(frames_to_sample=20,batch_size=20,num_epochs=20)\n",
    "conv_3d9_model=conv_3d9.define_model(dense_neurons=256,dropout=0.5)\n",
    "conv_3d9_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf509f-35c9-407c-9c9f-785ffb2bdf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Params:\", conv_3d9_model.count_params())\n",
    "history_model9=conv_3d9.train_model(conv_3d9_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbac4e9c-319c-4406-9f32-c83bfb25522f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the Model 3, the Model 10 has an augmentation (2,2,2) filter and a 120x120 image resolution.\n",
    "\n",
    "# Define the ModelConv3D10 class inheriting from ModelBuilderMoreAugmentation\n",
    "class ModelConv3D10(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    # Define the model with parameters for filter size, dense neurons, and dropout rate\n",
    "    def define_model(self, filter_size=(3, 3, 3), dense_neurons=64, dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # First Conv3D layer with 16 filters\n",
    "        model.add(Conv3D(16, filter_size, padding='same',\n",
    "                         input_shape=(self.frames_to_sample, self.image_height, self.image_width, self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Second Conv3D layer with 32 filters\n",
    "        model.add(Conv3D(32, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Third Conv3D layer with 64 filters\n",
    "        model.add(Conv3D(64, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Fourth Conv3D layer with 128 filters\n",
    "        model.add(Conv3D(128, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Flatten the 3D output to 1D\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # First fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Second fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer with softmax activation for multi-class classification\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        # Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a4e5a-bf39-4e72-830e-e2a0e4e6d49f",
   "metadata": {},
   "source": [
    "**Expected Changes and Impact**\n",
    " **1.Image Resolution (160x160 vs. 120x120):**\n",
    "\n",
    "- **Model 9 Impact:** Captures more spatial details, leading to better performance for datasets where fine-grained features are essential.\n",
    "- **Model 10 Impact:** Lower resolution reduces computational overhead, making it more suitable for faster training and datasets where high resolution isn't critical.\n",
    "  \n",
    " **2.Filter Size (3,3,3 vs. 2,2,2):**\n",
    "\n",
    "- **Model 9 Impact:** Larger filters cover a wider receptive field, allowing the model to capture more global patterns in fewer layers.\n",
    "- **Model 10 Impact:** Smaller filters focus on local features and require deeper architectures to achieve the same level of abstraction.\n",
    "  \n",
    " **3.Augmentation (Assumed Difference):**\n",
    "\n",
    "- **Model 9:** More aggressive augmentation introduces robustness to noise and variations, improving generalization.\n",
    "- **Model 10:** May rely on less augmentation, which can lead to faster training but slightly reduced robustness to data variations.\n",
    "  \n",
    " **4.Resource Requirements:**\n",
    "\n",
    "- **Model 9:** Requires more GPU memory and processing time due to higher resolution and more complex filters.\n",
    "- **Model 10:** More lightweight, suitable for scenarios with limited computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90c375-aeee-4d90-99ce-fdc541c0db8d",
   "metadata": {},
   "source": [
    "| **Scenario**                     | **ModelConv3D9**                    | **ModelConv3D10**                    |\n",
    "|----------------------------------|-------------------------------------|-------------------------------------|\n",
    "| **High-resolution video input**   | Recommended for extracting finer details. | May miss finer spatial details.    |\n",
    "| **Low-resolution datasets**       | Overkill for such cases.             | Ideal for low-resolution datasets.  |\n",
    "| **Limited computational resources** | May cause resource bottlenecks.      | More efficient for constrained resources. |\n",
    "| **Dataset with large variations**  | Augmentation helps generalization.   | Less robust to variations.          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8815d30-431a-4f25-849b-bfec008842f1",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "- **ModelConv3D9:** Best suited for high-resolution datasets or cases requiring fine-grained feature extraction but at the cost of increased computational resources.\n",
    "- **ModelConv3D10:** A lightweight alternative for datasets with lower resolutions or computational constraints, but may sacrifice performance in highly detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc33efa5-6577-4f11-b29c-28c30e01bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_3d10=ModelConv3D10()\n",
    "conv_3d10.initialize_path(project_folder)\n",
    "conv_3d10.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d10.initialize_hyperparams(frames_to_sample=16,batch_size=30,num_epochs=25)\n",
    "conv_3d10_model=conv_3d10.define_model(filtersize=(2,2,2),dense_neurons=256,dropout=0.5)\n",
    "conv_3d10_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6b7b6-99dc-405b-a019-f340986436b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Params:\", conv_3d10_model.count_params())\n",
    "history_model10=conv_3d10.train_model(conv_3d10_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a611de96-ab80-4733-a01b-b0423766b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_model10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be070b2a-78a0-43ce-8a27-b79231ec2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 11 with Augmentation\n",
    "# Adding more layers - Similar to model 4\n",
    "\n",
    "# Define the ModelConv3D11 class inheriting from ModelBuilderMoreAugmentation\n",
    "class ModelConv3D11(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    # Define the model with parameters for filter size, dense neurons, and dropout rate\n",
    "    def define_model(self, filter_size=(3, 3, 3), dense_neurons=64, dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # First block: two Conv3D layers with 16 filters\n",
    "        model.add(Conv3D(16, filter_size, padding='same',\n",
    "                         input_shape=(self.frames_to_sample, self.image_height, self.image_width, self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(16, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Second block: two Conv3D layers with 32 filters\n",
    "        model.add(Conv3D(32, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(32, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Third block: two Conv3D layers with 64 filters\n",
    "        model.add(Conv3D(64, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(64, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Fourth block: two Conv3D layers with 128 filters\n",
    "        model.add(Conv3D(128, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(128, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Flatten the 3D output to 1D\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # First fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Second fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer with softmax activation for multi-class classification\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        # Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55e91c-2d74-4ca0-9ce0-1955a4fe0bde",
   "metadata": {},
   "source": [
    "# Comparison Between ModelConv3D10 and ModelConv3D11\n",
    "\n",
    "| **Feature**                     | **ModelConv3D10**                                           | **ModelConv3D11**                                           |\n",
    "|----------------------------------|------------------------------------------------------------|------------------------------------------------------------|\n",
    "| **Architecture**                | Standard architecture with **one Conv3D layer** per block. | Advanced architecture with **two Conv3D layers** per block. |\n",
    "| **Number of Parameters**        | Lower parameter count due to single Conv3D layers.         | Higher parameter count due to additional Conv3D layers in each block. |\n",
    "| **Feature Extraction**          | Less complex features extracted at each block.             | Extracts **more complex and detailed features** with additional layers. |\n",
    "| **Depth of Network**            | Shallower network with fewer operations per block.         | Deeper network, providing better **hierarchical feature extraction**. |\n",
    "| **Computational Requirements**  | Computationally lightweight, faster training.              | **Higher computational cost** and longer training times due to extra layers. |\n",
    "| **Overfitting Risk**            | Lower risk of overfitting due to reduced complexity.        | **Higher risk of overfitting** without sufficient regularization or data. |\n",
    "| **Generalization**              | Adequate generalization for simpler datasets.              | Better suited for complex datasets with diverse patterns.  |\n",
    "| **Use of Augmentation**         | Relies on standard augmentation for robustness.            | **Enhanced robustness** when combined with stronger augmentation. |\n",
    "\n",
    "### Key Notes:\n",
    "- **ModelConv3D10** is ideal for simpler datasets and resource-constrained environments.\n",
    "- **ModelConv3D11** is better for complex datasets but requires more computational power and careful regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e60f529-16e0-411d-bc52-d6578edf5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_3d11=ModelConv3D11()\n",
    "conv_3d11.initialize_path(project_folder)\n",
    "conv_3d11.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d11.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=25)\n",
    "conv_3d11_model=conv_3d11.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.5)\n",
    "conv_3d11_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd1616c-0b96-4c00-8aee-1cb787aa7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Params:\", conv_3d11_model.count_params())\n",
    "history_model11=conv_3d11.train_model(conv_3d11_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3bcaf8-3de9-4abe-a0ba-b8522d28c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_model11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f26000-4d26-4715-a129-9fda046fce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 12 with Augmentation\n",
    "# Adding dropouts - Similar to Model 5\n",
    "\n",
    "# Define the ModelConv3D12 class inheriting from ModelBuilderMoreAugmentation\n",
    "class ModelConv3D12(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    # Define the model with parameters for filter size, dense neurons, and dropout rate\n",
    "    def define_model(self, filter_size=(3, 3, 3), dense_neurons=64, dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # First block: two Conv3D layers with 16 filters\n",
    "        model.add(Conv3D(16, filter_size, padding='same',\n",
    "                         input_shape=(self.frames_to_sample, self.image_height, self.image_width, self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(16, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Second block: two Conv3D layers with 32 filters\n",
    "        model.add(Conv3D(32, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(32, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Third block: two Conv3D layers with 64 filters\n",
    "        model.add(Conv3D(64, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(64, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Fourth block: two Conv3D layers with 128 filters\n",
    "        model.add(Conv3D(128, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv3D(128, filter_size, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Flatten the 3D output to 1D\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # First fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Second fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer with softmax activation for multi-class classification\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        # Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4c3507-6ae8-450a-bd2e-bc8263ba892f",
   "metadata": {},
   "source": [
    "**Summary of ModelConv3D12**\n",
    "- **Purpose:** Model 12 builds on Model 5 by introducing dropout layers after each MaxPooling3D layer to prevent overfitting.\n",
    "**Key Features:**\n",
    " **1.Architecture:**\n",
    "\n",
    "- Four blocks of **two Conv3D layers** each, followed by MaxPooling3D for downsampling.\n",
    "- Dropout (rate: 0.25) added after every pooling layer and dense layer for\n",
    "regularization.\n",
    "- Fully connected layers (2 Dense layers) to combine features.\n",
    "\n",
    "  \n",
    " **2.Regularization:**\n",
    "\n",
    "- **Dropout** layers after pooling layers and dense layers help reduce overfitting by randomly deactivating neurons.\n",
    "  \n",
    " **3.Hyperparameters:**\n",
    "\n",
    "- **Filter size:** (3, 3, 3) across all Conv3D layers.\n",
    "- **Dense neurons:** 64, with dropout applied in dense layers too.\n",
    "  \n",
    " **4.Compilation:**\n",
    "\n",
    "- **Optimizer:** Adam with a learning rate of 0.0002.\n",
    "- **Loss:** Categorical crossentropy for multi-class classification.\n",
    "- **Metrics:** Categorical accuracy.\n",
    "\n",
    " **5.Augmentation Suitability:**\n",
    "\n",
    "- This model is designed to perform well on augmented data by improving robustness through dropouts.\n",
    "  \n",
    "**Use Case:**\n",
    "- Suitable for datasets prone to **overfitting** or when dealing with smaller datasets.\n",
    "- Enhances generalization by preventing over-reliance on specific features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffb67be-972a-4f44-94e0-5caaa5d73b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_3d12=ModelConv3D12()\n",
    "conv_3d12.initialize_path(project_folder)\n",
    "conv_3d12.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d12.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=25)\n",
    "conv_3d12_model=conv_3d12.define_model(filtersize=(3,3,3),dense_neurons=256,dropout=0.25)\n",
    "conv_3d12_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b2400-fdc8-40c2-bb03-2540477c5085",
   "metadata": {},
   "source": [
    "**Explanation of Code:**\n",
    "- **1.Model Initialization**\n",
    "- **2.Model Definition**\n",
    "**Key Points:**\n",
    "- **Resolution:** 120x120 image size.\n",
    "- **Hyperparameters:** 16 frames, batch size 20, 25 epochs.\n",
    "- **Model:** Defines a 3D convolutional network with large dense layers (256 neurons).\n",
    "- **Dropout:** Added for regularization (0.25).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e669e7c7-cec4-4cdf-98f1-3f69614b3485",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Params:\", conv_3d12_model.count_params())\n",
    "history_model12=conv_3d12.train_model(conv_3d12_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036ea362-f1aa-456e-ba0c-826c82fc41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_model12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573c4c6-c46d-4427-9a19-e4e4bfb47218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparable to Model 6, Model 13 with Augmentation Reducing network parameters\n",
    "\n",
    "# Define the ModelConv3D13 class inheriting from ModelBuilderMoreAugmentation\n",
    "class ModelConv3D13(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    # Define the model with parameters for dense neurons and dropout rate\n",
    "    def define_model(self, dense_neurons=64, dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # First convolutional block with filter size (3, 3, 3)\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
    "                         input_shape=(self.frames_to_sample, self.image_height, self.image_width, self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Second convolutional block with filter size (2, 2, 2)\n",
    "        model.add(Conv3D(32, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Third convolutional block with filter size (2, 2, 2)\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Fourth convolutional block with filter size (2, 2, 2)\n",
    "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "        # Flatten the 3D output to 1D\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # First fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Second fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer with softmax activation for multi-class classification\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        # Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3dd2cc-1a56-40e7-97d1-e204678c24fa",
   "metadata": {},
   "source": [
    "**Summary of ModelConv3D13**\n",
    "- **Purpose:** Model 13 reduces the network parameters compared to Model 6 by using smaller filters in the Conv3D layers.\n",
    "  \n",
    "**Key Features:**\n",
    " **1.Architecture:**\n",
    "\n",
    "- **Four convolutional blocks:**\n",
    "- 1st block: Conv3D with 16 filters (3x3x3), MaxPooling3D (2x2x2).\n",
    "- 2nd, 3rd, 4th blocks: Conv3D with 32, 64, and 128 filters (2x2x2), MaxPooling3D.\n",
    "- **BatchNormalization and ReLU activation** after each convolution.\n",
    "- **Flatten** the 3D output for the fully connected layer.\n",
    "  \n",
    " **2.Fully Connected Layers:**\n",
    "\n",
    "- Two Dense layers (64 neurons) with dropout (0.25) for regularization.\n",
    "  \n",
    " **3.Regularization:**\n",
    "\n",
    "- **Dropout** after dense layers to prevent overfitting.\n",
    "  \n",
    " **4.Compilation:**\n",
    "\n",
    "- **Optimizer:** Adam with learning rate 0.0002.\n",
    "- **Loss:** Categorical crossentropy.\n",
    "- **Metrics:** Categorical accuracy.\n",
    "**Key Differences:\n",
    "- **Smaller filters (2x2x2):** In contrast to larger filter sizes in previous models, reducing parameters.\n",
    "- **Efficiency:** Aims to balance performance with fewer parameters, making it suitable for faster training and lower computational cost.\n",
    "  \n",
    "**Use Case:**\n",
    "- Best suited for smaller datasets or limited computational resources.\n",
    "- Can be used when model simplicity is prioritized without compromising much on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de93fbc9-5f3c-4c5a-b6e0-53c8e809321e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the ModelConv3D13 class\n",
    "conv_3d13 = ModelConv3D13()\n",
    "\n",
    "# Initialize paths and image properties\n",
    "conv_3d13.initialize_path(project_folder)\n",
    "conv_3d13.initialize_image_properties(image_height=100, image_width=100)\n",
    "conv_3d13.initialize_hyperparams(frames_to_sample=16, batch_size=20, num_epochs=25)\n",
    "\n",
    "# Define the ModelConv3D13 model with specified parameters\n",
    "conv_3d13_model = conv_3d13.define_model(dense_neurons=128, dropout=0.25)\n",
    "\n",
    "# Display model summary\n",
    "conv_3d13_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b529b-24da-49c0-b4b5-01f026ce783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Params:\", conv_3d13_model.count_params())\n",
    "history_model13=conv_3d13.train_model(conv_3d13_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71000562-bdd3-4047-9647-447b4ddd20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_model13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd3fe7-47a6-4fd8-b480-dded4136389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ModelConv3D14 class inheriting from ModelBuilderMoreAugmentation\n",
    "class ModelConv3D14(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    # Define the model with parameters for dense neurons and dropout rate\n",
    "    def define_model(self, dense_neurons=64, dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        # First convolutional block with filter size (3, 3, 3)\n",
    "        model.add(Conv3D(16, (3, 3, 3), padding='same',\n",
    "                         input_shape=(self.frames_to_sample, self.image_height, self.image_width, self.channels)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Second convolutional block with filter size (3, 3, 3)\n",
    "        model.add(Conv3D(32, (3, 3, 3), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Third convolutional block with filter size (2, 2, 2)\n",
    "        model.add(Conv3D(64, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "        # Fourth convolutional block with filter size (2, 2, 2)\n",
    "        model.add(Conv3D(128, (2, 2, 2), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "        \n",
    "        # Flatten the 3D output to 1D\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # First fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Second fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        # Output layer with softmax activation for multi-class classification\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        # Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f866543e-a060-4c5a-8416-d537b45a96ac",
   "metadata": {},
   "source": [
    "### Comparison Between ModelConv3D13 and ModelConv3D14\n",
    "\n",
    "| **Aspect**             | **ModelConv3D13**                                          | **ModelConv3D14**                                          | **Difference/Impact**                                                                                     |\n",
    "|------------------------|------------------------------------------------------------|------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| **Convolutional Layers**| 4 Conv3D blocks with varying filter sizes (16, 32, 64, 128) | 4 Conv3D blocks: 2 with filter size (3x3x3), 2 with (2x2x2) | **ModelConv3D14** uses larger (3x3x3) filters in the first two blocks and smaller (2x2x2) in the last two. |\n",
    "| **Filter Size**         | (3, 3, 3) for all layers                                   | (3, 3, 3) for first two layers, (2, 2, 2) for last two     | **ModelConv3D14** reduces parameter count in last two layers, which may reduce overfitting and training time. |\n",
    "| **Dense Layers**        | 2 Dense layers with 64 neurons each                        | 2 Dense layers with 64 neurons each                        | Same architecture, no change.                                                                               |\n",
    "| **Dropout**             | Dropout after each block and dense layer                   | Dropout after each block and dense layer                   | **No difference** in dropout rate (0.25).                                                                  |\n",
    "| **Output Layer**        | Softmax for multi-class classification                     | Softmax for multi-class classification                     | **No difference**.                                                                                         |\n",
    "| **Total Parameters**    | Higher due to more parameters in Conv3D layers             | Slightly fewer due to reduced size in last two Conv3D layers | **ModelConv3D14** may be slightly faster to train due to reduced parameters.                               |\n",
    "| **MaxPooling3D**        | MaxPooling3D after each Conv3D block                       | MaxPooling3D after each Conv3D block                       | **No difference** in pooling layers.                                                                       |\n",
    "\n",
    "### Key Differences and Their Impact:\n",
    "1. **Filter Sizes**: \n",
    "   - **ModelConv3D14** uses smaller filters (2x2x2) in the last two convolutional layers compared to **ModelConv3D13**, which uses uniform (3x3x3) filters across all layers. This results in fewer parameters in **ModelConv3D14**, potentially leading to quicker training times and reduced computational cost.\n",
    "   \n",
    "2. **Parameter Count**:\n",
    "   - The reduction in the filter size in **ModelConv3D14** leads to a decrease in the total number of parameters compared to **ModelConv3D13**, which can help with overfitting on smaller datasets.\n",
    "\n",
    "3. **Model Complexity**:\n",
    "   - **ModelConv3D13** is more complex, with a uniform filter size across all layers, allowing it to capture more detailed features. However, this can lead to slower training and the risk of overfitting.\n",
    "   - **ModelConv3D14** is designed to be more efficient by reducing parameters, making it potentially better suited for situations where model training time or computational resources are limited.\n",
    "\n",
    "### Use Case Considerations:\n",
    "- **ModelConv3D14** may be more suitable for smaller datasets or when faster training is needed due to its reduced number of parameters.\n",
    "- **ModelConv3D13**, being more parameter-heavy, could perform better on large, complex datasets where it can take full advantage of the deeper feature extraction capabilities provided by the uniform filter sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166411b-8816-4edb-94ed-f4f71933e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conv_3d14=ModelConv3D14()\n",
    "conv_3d14.initialize_path(project_folder)\n",
    "conv_3d14.initialize_image_properties(image_height=120,image_width=120)\n",
    "conv_3d14.initialize_hyperparams(frames_to_sample=16,batch_size=20,num_epochs=25)\n",
    "conv_3d14_model=conv_3d14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a69fcc-461d-4e3c-bacc-e340dac918b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Params:\", conv_3d14_model.count_params())\n",
    "history_model14=conv_3d14.train_model(conv_3d14_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d2e71-87f2-4e33-9c06-63359073dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_model14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fcc2f1-fbba-4b26-b39c-5da87c578795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 15 with Augmentation\n",
    "# CNN LSTM with GRU - Similar to Model 8\n",
    "\n",
    "# Define the RNNCNN2 class inheriting from ModelBuilderMoreAugmentation\n",
    "class RNNCNN2(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    # Define the model with parameters for LSTM cells, dense neurons, and dropout rate\n",
    "    def define_model(self, lstm_cells=64, dense_neurons=64, dropout=0.25):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # First TimeDistributed convolutional block\n",
    "        model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "                                  input_shape=(self.frames_to_sample, self.image_height, self.image_width, self.channels)))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        # Second TimeDistributed convolutional block\n",
    "        model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        # Third TimeDistributed convolutional block\n",
    "        model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        # Fourth TimeDistributed convolutional block\n",
    "        model.add(TimeDistributed(Conv2D(128, (3, 3), padding='same', activation='relu')))\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        # Flatten the 2D output to 1D for input to the RNN layer\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        # GRU layer to capture temporal dependencies\n",
    "        model.add(GRU(lstm_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        # First fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        # Second fully connected dense layer\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "\n",
    "        # Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "        optimiser = optimizers.Adam(lr=0.0002)\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a5b73-a7be-4a67-a6a2-3650aa6e4b0e",
   "metadata": {},
   "source": [
    "# RNNCNN2 Model - Combining CNN, GRU, and Dense Layers\n",
    "\n",
    "The RNNCNN2 model is a hybrid neural network architecture for processing sequential data like videos. It leverages **CNNs** for spatial feature extraction and **GRU** for temporal sequence modeling. Here's a breakdown of the key components:\n",
    "\n",
    "1. **TimeDistributed CNN Blocks**:\n",
    "   - Extract spatial features from each frame.\n",
    "   - Normalization and pooling ensure efficient feature learning.\n",
    "\n",
    "2. **GRU Layer**:\n",
    "   - Captures temporal relationships between video frames.\n",
    "\n",
    "3. **Dense Layers**:\n",
    "   - Process the temporal features for classification.\n",
    "   - Includes dropout layers to prevent overfitting.\n",
    "\n",
    "4. **Key Differences from Model 14**:\n",
    "   - GRU replaces LSTM, making it computationally efficient.\n",
    "   - An additional dense layer is added for better feature learning.\n",
    "\n",
    "5. **Compilation**:\n",
    "   - Uses the Adam optimizer with a learning rate of 0.0002.\n",
    "   - Categorical crossentropy as the loss function, suitable for multi-class problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7d03c-ddfd-4ed9-93d2-240f37b1ffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cnn2=RNNCNN2()\n",
    "rnn_cnn2.initialize_path(project_folder)\n",
    "rnn_cnn2.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn2.initialize_hyperparams(frames_to_sample=18,batch_size=20,num_epochs=20)\n",
    "rnn_cnn2_model=rnn_cnn2.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5e707-822c-419d-b3c8-04e2024942dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Params:\", rnn_cnn2_model.count_params())\n",
    "history_model15=rnn_cnn2.train_model(rnn_cnn2_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a16f2-47ef-4c32-8c3b-562da9e6ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_model15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b9499b-b7b0-425d-bc31-21d904b8fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the MobileNet model in place of more powerful models like VGG16, Alexnet, InceptionV3, etc. because of its lightweight construction and fast performance. \n",
    "# Furthermore, we are currently experiencing low disk space on the nimblebox.ai platform. \n",
    "\n",
    "from keras.applications import mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81519d1-a919-49ca-a621-1760b53e8d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 16 \n",
    "\n",
    "# Import MobileNet and instantiate with pre-trained weights\n",
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "# Define the RNNCNN_TL class inheriting from ModelBuilderMoreAugmentation\n",
    "class RNNCNN_TL(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    # Define the model with parameters for LSTM cells, dense neurons, and dropout rate\n",
    "    def define_model(self, lstm_cells=64, dense_neurons=64, dropout=0.25):\n",
    "        \n",
    "        # Initialize a Sequential model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Add TimeDistributed MobileNet layer as the convolutional base\n",
    "        model.add(TimeDistributed(mobilenet_transfer, input_shape=(self.frames_to_sample, self.image_height, self.image_width, self.channels)))\n",
    "        \n",
    "        # Freeze MobileNet layers to prevent training\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = False\n",
    "        \n",
    "        # Batch normalization and max pooling layers\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        \n",
    "        # Flatten the output for input to LSTM layer\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        # LSTM layer to capture temporal dependencies\n",
    "        model.add(LSTM(lstm_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        # First fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        # Second fully connected dense layer\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        # Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb5d814-2cf1-49d4-b687-c07341609000",
   "metadata": {},
   "source": [
    "# RNNCNN_TL Model - Transfer Learning with MobileNet and LSTM\n",
    "\n",
    "The RNNCNN_TL model utilizes **transfer learning** to combine the spatial feature extraction power of a pre-trained MobileNet and the temporal modeling capabilities of an LSTM layer. Below are the key components:\n",
    "\n",
    "1. **MobileNet Transfer Learning**:\n",
    "   - MobileNet is pre-trained on ImageNet and used as a feature extractor.\n",
    "   - The top classification layers of MobileNet are removed, and the weights are frozen to retain its pre-trained features.\n",
    "\n",
    "2. **TimeDistributed Wrapper**:\n",
    "   - Ensures that MobileNet processes each video frame independently while preserving the sequence.\n",
    "\n",
    "3. **BatchNormalization and MaxPooling**:\n",
    "   - Normalize the extracted features and reduce spatial dimensions.\n",
    "\n",
    "4. **Flattening**:\n",
    "   - Converts 2D feature maps into 1D vectors for the LSTM layer.\n",
    "\n",
    "5. **LSTM Layer**:\n",
    "   - Captures temporal dependencies across the sequence of frames.\n",
    "\n",
    "6. **Fully Connected Dense Layers**:\n",
    "   - First dense layer refines features with ReLU activation.\n",
    "   - Second dense layer outputs probabilities for each class using softmax activation.\n",
    "\n",
    "7. **Key Characteristics**:\n",
    "   - Leverages transfer learning for efficient spatial feature extraction.\n",
    "   - Regularization with dropout to prevent overfitting.\n",
    "   - Uses Adam optimizer and categorical crossentropy loss for training.\n",
    "\n",
    "This architecture is well-suited for video classification tasks, combining the strengths of pre-trained CNNs and RNNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542aa0a9-efb6-4abe-acb1-67e811f87e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_cnn_tl=RNNCNN_TL()\n",
    "rnn_cnn_tl.initialize_path(project_folder)\n",
    "rnn_cnn_tl.initialize_image_properties(image_height=120,image_width=120)\n",
    "rnn_cnn_tl.initialize_hyperparams(frames_to_sample=16,batch_size=5,num_epochs=20)\n",
    "rnn_cnn_tl_model=rnn_cnn_tl.define_model(lstm_cells=128,dense_neurons=128,dropout=0.25)\n",
    "rnn_cnn_tl_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bb905f-0286-4e41-a970-96e862969efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Params:\", rnn_cnn_tl_model.count_params())\n",
    "history_model16=rnn_cnn_tl.train_model(rnn_cnn_tl_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad21675-2b92-499c-b3d4-f4ffa9b6be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_model16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c0cece-7b69-49e0-986d-9ba3d021fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MobileNet from Keras applications\n",
    "from keras.applications import mobilenet\n",
    "\n",
    "# Load MobileNet pre-trained on ImageNet\n",
    "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "\n",
    "# Define the RNNCNN_TL2 class inheriting from ModelBuilderMoreAugmentation\n",
    "class RNNCNN_TL2(ModelBuilderMoreAugmentation):\n",
    "    \n",
    "    # Define the model with parameters for GRU cells, dense neurons, and dropout rate\n",
    "    def define_model(self, gru_cells=64, dense_neurons=64, dropout=0.25):\n",
    "        \n",
    "        # Initialize a Sequential model\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Add TimeDistributed MobileNet layer as the convolutional base\n",
    "        model.add(TimeDistributed(mobilenet_transfer, input_shape=(self.frames_to_sample, self.image_height, self.image_width, self.channels)))\n",
    " \n",
    "        # Batch normalization and max pooling layers\n",
    "        model.add(TimeDistributed(BatchNormalization()))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "        # GRU layer to capture temporal dependencies\n",
    "        model.add(GRU(gru_cells))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        # First fully connected dense layer\n",
    "        model.add(Dense(dense_neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "        \n",
    "        # Second fully connected dense layer\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        \n",
    "        # Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "        optimiser = optimizers.Adam()\n",
    "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c344b-f52c-42eb-be40-0e9c04ba9d9e",
   "metadata": {},
   "source": [
    "# RNNCNN_TL2 Model - Transfer Learning with MobileNet and GRU\n",
    "\n",
    "The **RNNCNN_TL2** model is an enhanced architecture that combines **MobileNet** as a pre-trained convolutional base and a **GRU** (Gated Recurrent Unit) for modeling temporal dependencies in sequential data. This model is particularly effective for tasks like video classification. Here's a breakdown of its components:\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Components of RNNCNN_TL2**\n",
    "\n",
    "1. **MobileNet as Convolutional Base**:\n",
    "   - **Purpose**: Leverage the spatial feature extraction power of MobileNet pre-trained on the ImageNet dataset.\n",
    "   - **Details**:\n",
    "     - The MobileNet model is imported from Keras and its top classification layers are excluded (`include_top=False`).\n",
    "     - MobileNet processes individual frames of a video independently using the `TimeDistributed` wrapper.\n",
    "\n",
    "2. **TimeDistributed Wrapper**:\n",
    "   - **Purpose**: Ensure that MobileNet and subsequent layers process each frame in a sequence independently.\n",
    "   - **Details**:\n",
    "     - Wraps MobileNet and subsequent layers to maintain the temporal sequence structure.\n",
    "\n",
    "3. **BatchNormalization and MaxPooling**:\n",
    "   - **Purpose**:\n",
    "     - `BatchNormalization`: Normalizes feature activations, improving convergence and stabilizing training.\n",
    "     - `MaxPooling2D`: Reduces spatial dimensions to focus on the most salient features.\n",
    "\n",
    "4. **Flatten Layer**:\n",
    "   - **Purpose**: Converts the 2D feature maps from MobileNet into 1D vectors for input to the GRU layer.\n",
    "\n",
    "5. **GRU Layer**:\n",
    "   - **Purpose**: Captures temporal dependencies and patterns across the sequence of frames.\n",
    "   - **Details**:\n",
    "     - GRUs are efficient and perform well for sequences with temporal relationships.\n",
    "     - The number of GRU units (`gru_cells`) is configurable (default: 64).\n",
    "\n",
    "6. **Dropout Layers**:\n",
    "   - **Purpose**: Prevent overfitting by randomly dropping connections during training.\n",
    "\n",
    "7. **Fully Connected Dense Layers**:\n",
    "   - **Purpose**:\n",
    "     - The first dense layer learns high-level representations using ReLU activation.\n",
    "     - The second dense layer outputs class probabilities using `softmax` activation.\n",
    "   - **Details**:\n",
    "     - Configurable number of neurons in the first dense layer (`dense_neurons`, default: 64).\n",
    "\n",
    "8. **Compilation**:\n",
    "   - **Optimizer**: Adam optimizer for adaptive learning rate optimization.\n",
    "   - **Loss Function**: Categorical Crossentropy, ideal for multi-class classification problems.\n",
    "   - **Metric**: Categorical accuracy to evaluate performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **MobileNet** provides powerful spatial feature extraction without the need for extensive training.\n",
    "- **GRU** models temporal relationships efficiently with fewer parameters compared to LSTMs.\n",
    "- Regularization through **dropout layers** ensures robustness and prevents overfitting.\n",
    "- The model is well-suited for sequential tasks like video classification, combining the strengths of transfer learning and RNNs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263c9c1-2628-4fa3-b4dc-331500d1f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an instance of the RNNCNN_TL2 class\n",
    "rnn_cnn_tl2 = RNNCNN_TL2()\n",
    "\n",
    "# Initialize paths, image properties, and hyperparameters\n",
    "rnn_cnn_tl2.initialize_path(project_folder)\n",
    "rnn_cnn_tl2.initialize_image_properties(image_height=120, image_width=120)\n",
    "rnn_cnn_tl2.initialize_hyperparams(frames_to_sample=16, batch_size=5, num_epochs=20)\n",
    "\n",
    "# Define the RNNCNN_TL2 model with specified parameters\n",
    "rnn_cnn_tl2_model = rnn_cnn_tl2.define_model(gru_cells=128, dense_neurons=128, dropout=0.25)\n",
    "\n",
    "# Print the summary of the RNNCNN_TL2 model\n",
    "rnn_cnn_tl2_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655a781-78f1-447d-8155-cdbc5b824ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Params:\", rnn_cnn_tl2_model.count_params())\n",
    "history_model17=rnn_cnn_tl2.train_model(rnn_cnn_tl2_model,augment_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f3d0a-4d56-4fdd-93b6-efbdf3c5071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(history_model17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97321184-ba5a-4b19-be0b-0007dcd6df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "# The `load_model` function from Keras is used to load a pre-trained model from a specified file path.\n",
    "# In this case, the model file is named 'model-00001-1.34029-0.43137-1.75732-0.28000.h5' and is located\n",
    "# within the directory 'model_init_2024-05-2816_20_05.434836'. The numbers in the file name might represent\n",
    "# different metrics such as loss and accuracy during training, typically indicating epoch number, training\n",
    "# loss, training accuracy, validation loss, and validation accuracy respectively.\n",
    "model = load_model('model_init_2024-06-0407_48_54.745934/model-00001-1.34029-0.43137-1.75732-0.28000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f6eadf-efb0-42e3-bbdb-f2da0bfe481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the test generator\n",
    "# `RNNCNN1` is a class that is presumably defined elsewhere in your code. It is used to create an instance of a data generator\n",
    "# for testing purposes. This generator likely prepares and feeds data in a specific format suitable for RNN and CNN models.\n",
    "test_generator = RNNCNN1()\n",
    "\n",
    "# Initialize the path for the test generator\n",
    "# The `initialize_path` method is called with `project_folder` to set up the directory or path where the data is located.\n",
    "test_generator.initialize_path(project_folder)\n",
    "\n",
    "# Set image properties\n",
    "# The `initialize_image_properties` method sets the height and width of the images that the generator will process.\n",
    "# Here, the images are set to be 120x120 pixels.\n",
    "test_generator.initialize_image_properties(image_height=120, image_width=120)\n",
    "\n",
    "# Set hyperparameters\n",
    "# The `initialize_hyperparams` method sets various hyperparameters for the generator.\n",
    "# - `frames_to_sample`: Number of frames to sample from the video or sequence.\n",
    "# - `batch_size`: Number of samples per batch of computation.\n",
    "# - `num_epochs`: Number of epochs for which the generator will run.\n",
    "test_generator.initialize_hyperparams(frames_to_sample=18, batch_size=20, num_epochs=20)\n",
    "\n",
    "# Create a data generator\n",
    "# The `generator` method of `test_generator` is called to create an actual data generator.\n",
    "# - `test_generator.val_path` and `test_generator.val_doc` presumably specify the validation data's path and documents respectively.\n",
    "# - `augment=False` indicates that data augmentation (like random transformations) is not applied.\n",
    "g = test_generator.generator(test_generator.val_path, test_generator.val_doc, augment=False)\n",
    "\n",
    "# Fetch the next batch of data\n",
    "# The `next` function is used to get the next batch of data from the generator `g`.\n",
    "# `batch_data` contains the data samples in the batch, and `batch_labels` contains the corresponding labels.\n",
    "batch_data, batch_labels = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f182c9-670b-4292-a46c-dbef14115e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be6e097-a518-4bb7-b46d-f707c2fd0f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
